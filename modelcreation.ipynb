{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelcreation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRqFLeWIb4bGY+h++fhLCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ar-ushi/Predicting-Next-Word/blob/main/modelcreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTzwWcczRquN"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import heapq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0P7XDuUZuU2"
      },
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "H53sDWc8bDND",
        "outputId": "7dd2fd9b-246c-46c9-bafe-0e63f14ff325"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0105d9dc-85fe-4f04-8db8-00d0a60f0362\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0105d9dc-85fe-4f04-8db8-00d0a60f0362\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Dataset.txt to Dataset (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfrP6PM_b-27"
      },
      "source": [
        "file = open('Dataset.txt', encoding='utf-8').read().lower()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qNuySa0cSTB"
      },
      "source": [
        "data = \" \" \n",
        "data= file.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FINOiTmycxAt"
      },
      "source": [
        "import string \n",
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
        "new_data = data.translate(translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_nmsBefdHE_"
      },
      "source": [
        "z = []\n",
        "\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "        \n",
        "data = ' '.join(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14FbgurhduRT"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "dict_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "sequences = np.array(sequences)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "y = to_categorical(y, num_classes=dict_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jb8emMdeoZ9"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(dict_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(dict_size, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSRaiqqufQqb",
        "outputId": "28f600ce-2955-4720-e244-24cda981bec8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             226880    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 22688)             22710688  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 35,986,568\n",
            "Trainable params: 35,986,568\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEovUn5Yfr5R"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"predictingword.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9ETCzRBSzvv",
        "outputId": "19ecea6a-0aea-4c3c-cd68-4198377f6716"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam \n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))\n",
        "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 9.6373\n",
            "Epoch 00001: loss improved from inf to 9.63727, saving model to predictingword.h5\n",
            "703/703 [==============================] - 35s 37ms/step - loss: 9.6373 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 9.2315\n",
            "Epoch 00002: loss improved from 9.63727 to 9.23151, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 9.2315 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 8.9552\n",
            "Epoch 00003: loss improved from 9.23151 to 8.95572, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 8.9557 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 8.6218\n",
            "Epoch 00004: loss improved from 8.95572 to 8.62177, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 8.6218 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 8.2397\n",
            "Epoch 00005: loss improved from 8.62177 to 8.23972, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 8.2397 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 7.7753\n",
            "Epoch 00006: loss improved from 8.23972 to 7.77530, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 7.7753 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 7.2485\n",
            "Epoch 00007: loss improved from 7.77530 to 7.24847, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 7.2485 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 6.6668\n",
            "Epoch 00008: loss improved from 7.24847 to 6.66684, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 6.6668 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 6.0529\n",
            "Epoch 00009: loss improved from 6.66684 to 6.05286, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 6.0529 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 5.4663\n",
            "Epoch 00010: loss improved from 6.05286 to 5.46631, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 5.4663 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 4.9298\n",
            "Epoch 00011: loss improved from 5.46631 to 4.92981, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 4.9298 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 4.4772\n",
            "Epoch 00012: loss improved from 4.92981 to 4.47718, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 4.4772 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 4.0931\n",
            "Epoch 00013: loss improved from 4.47718 to 4.09312, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 4.0931 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 3.7909\n",
            "Epoch 00014: loss improved from 4.09312 to 3.79093, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 3.7909 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 3.5506\n",
            "Epoch 00015: loss improved from 3.79093 to 3.55059, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 3.5506 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 3.3586\n",
            "Epoch 00016: loss improved from 3.55059 to 3.35871, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 3.3587 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 3.1975\n",
            "Epoch 00017: loss improved from 3.35871 to 3.19749, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 3.1975 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 3.0724\n",
            "Epoch 00018: loss improved from 3.19749 to 3.07241, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 3.0724 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.9676\n",
            "Epoch 00019: loss improved from 3.07241 to 2.96762, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.9676 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.8726\n",
            "Epoch 00020: loss improved from 2.96762 to 2.87262, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.8726 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.7909\n",
            "Epoch 00021: loss improved from 2.87262 to 2.79089, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.7909 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.7149\n",
            "Epoch 00022: loss improved from 2.79089 to 2.71495, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.7149 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 2.6538\n",
            "Epoch 00023: loss improved from 2.71495 to 2.65402, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.6540 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.5963\n",
            "Epoch 00024: loss improved from 2.65402 to 2.59628, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.5963 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.5527\n",
            "Epoch 00025: loss improved from 2.59628 to 2.55275, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.5527 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.5093\n",
            "Epoch 00026: loss improved from 2.55275 to 2.50934, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.5093 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.4642\n",
            "Epoch 00027: loss improved from 2.50934 to 2.46418, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.4642 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.4263\n",
            "Epoch 00028: loss improved from 2.46418 to 2.42635, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.4263 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 2.3963\n",
            "Epoch 00029: loss improved from 2.42635 to 2.39676, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.3968 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.3650\n",
            "Epoch 00030: loss improved from 2.39676 to 2.36504, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.3650 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 2.3307\n",
            "Epoch 00031: loss improved from 2.36504 to 2.33081, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 2.3308 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.3069\n",
            "Epoch 00032: loss improved from 2.33081 to 2.30687, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 2.3069 - lr: 0.0010\n",
            "Epoch 33/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.2832\n",
            "Epoch 00033: loss improved from 2.30687 to 2.28321, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.2832 - lr: 0.0010\n",
            "Epoch 34/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 2.2562\n",
            "Epoch 00034: loss improved from 2.28321 to 2.25625, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 2.2563 - lr: 0.0010\n",
            "Epoch 35/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.2352\n",
            "Epoch 00035: loss improved from 2.25625 to 2.23522, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.2352 - lr: 0.0010\n",
            "Epoch 36/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 2.2170\n",
            "Epoch 00036: loss improved from 2.23522 to 2.21690, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.2169 - lr: 0.0010\n",
            "Epoch 37/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.1941\n",
            "Epoch 00037: loss improved from 2.21690 to 2.19406, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.1941 - lr: 0.0010\n",
            "Epoch 38/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 2.1776\n",
            "Epoch 00038: loss improved from 2.19406 to 2.17795, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.1780 - lr: 0.0010\n",
            "Epoch 39/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.1620\n",
            "Epoch 00039: loss improved from 2.17795 to 2.16197, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.1620 - lr: 0.0010\n",
            "Epoch 40/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 2.1434\n",
            "Epoch 00040: loss improved from 2.16197 to 2.14355, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.1436 - lr: 0.0010\n",
            "Epoch 41/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 2.1286\n",
            "Epoch 00041: loss improved from 2.14355 to 2.12809, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.1281 - lr: 0.0010\n",
            "Epoch 42/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.1142\n",
            "Epoch 00042: loss improved from 2.12809 to 2.11418, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 2.1142 - lr: 0.0010\n",
            "Epoch 43/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.0998\n",
            "Epoch 00043: loss improved from 2.11418 to 2.09983, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 2.0998 - lr: 0.0010\n",
            "Epoch 44/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 2.0849\n",
            "Epoch 00044: loss improved from 2.09983 to 2.08547, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 2.0855 - lr: 0.0010\n",
            "Epoch 45/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 2.0722\n",
            "Epoch 00045: loss improved from 2.08547 to 2.07231, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.0723 - lr: 0.0010\n",
            "Epoch 46/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.0620\n",
            "Epoch 00046: loss improved from 2.07231 to 2.06199, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.0620 - lr: 0.0010\n",
            "Epoch 47/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.0523\n",
            "Epoch 00047: loss improved from 2.06199 to 2.05229, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.0523 - lr: 0.0010\n",
            "Epoch 48/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.0387\n",
            "Epoch 00048: loss improved from 2.05229 to 2.03868, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.0387 - lr: 0.0010\n",
            "Epoch 49/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.0273\n",
            "Epoch 00049: loss improved from 2.03868 to 2.02727, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 2.0273 - lr: 0.0010\n",
            "Epoch 50/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.0159\n",
            "Epoch 00050: loss improved from 2.02727 to 2.01594, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 2.0159 - lr: 0.0010\n",
            "Epoch 51/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 2.0042\n",
            "Epoch 00051: loss improved from 2.01594 to 2.00416, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 2.0042 - lr: 0.0010\n",
            "Epoch 52/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.9941\n",
            "Epoch 00052: loss improved from 2.00416 to 1.99412, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 1.9941 - lr: 0.0010\n",
            "Epoch 53/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.9835\n",
            "Epoch 00053: loss improved from 1.99412 to 1.98354, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 1.9835 - lr: 0.0010\n",
            "Epoch 54/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.9781\n",
            "Epoch 00054: loss improved from 1.98354 to 1.97807, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.9781 - lr: 0.0010\n",
            "Epoch 55/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.9722\n",
            "Epoch 00055: loss improved from 1.97807 to 1.97223, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 1.9722 - lr: 0.0010\n",
            "Epoch 56/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.9591\n",
            "Epoch 00056: loss improved from 1.97223 to 1.95909, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.9591 - lr: 0.0010\n",
            "Epoch 57/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.9494\n",
            "Epoch 00057: loss improved from 1.95909 to 1.94934, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 1.9493 - lr: 0.0010\n",
            "Epoch 58/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.9445\n",
            "Epoch 00058: loss improved from 1.94934 to 1.94488, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.9449 - lr: 0.0010\n",
            "Epoch 59/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.9373\n",
            "Epoch 00059: loss improved from 1.94488 to 1.93732, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 1.9373 - lr: 0.0010\n",
            "Epoch 60/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.9282\n",
            "Epoch 00060: loss improved from 1.93732 to 1.92851, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.9285 - lr: 0.0010\n",
            "Epoch 61/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.9213\n",
            "Epoch 00061: loss improved from 1.92851 to 1.92132, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.9213 - lr: 0.0010\n",
            "Epoch 62/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.9125\n",
            "Epoch 00062: loss improved from 1.92132 to 1.91252, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.9125 - lr: 0.0010\n",
            "Epoch 63/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.9095\n",
            "Epoch 00063: loss improved from 1.91252 to 1.90954, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.9095 - lr: 0.0010\n",
            "Epoch 64/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8995\n",
            "Epoch 00064: loss improved from 1.90954 to 1.89950, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8995 - lr: 0.0010\n",
            "Epoch 65/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.8929\n",
            "Epoch 00065: loss improved from 1.89950 to 1.89303, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8930 - lr: 0.0010\n",
            "Epoch 66/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8890\n",
            "Epoch 00066: loss improved from 1.89303 to 1.88895, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8890 - lr: 0.0010\n",
            "Epoch 67/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.8820\n",
            "Epoch 00067: loss improved from 1.88895 to 1.88179, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8818 - lr: 0.0010\n",
            "Epoch 68/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8753\n",
            "Epoch 00068: loss improved from 1.88179 to 1.87531, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8753 - lr: 0.0010\n",
            "Epoch 69/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8716\n",
            "Epoch 00069: loss improved from 1.87531 to 1.87162, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8716 - lr: 0.0010\n",
            "Epoch 70/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8646\n",
            "Epoch 00070: loss improved from 1.87162 to 1.86460, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8646 - lr: 0.0010\n",
            "Epoch 71/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8585\n",
            "Epoch 00071: loss improved from 1.86460 to 1.85852, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8585 - lr: 0.0010\n",
            "Epoch 72/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8534\n",
            "Epoch 00072: loss improved from 1.85852 to 1.85342, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8534 - lr: 0.0010\n",
            "Epoch 73/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8489\n",
            "Epoch 00073: loss improved from 1.85342 to 1.84893, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8489 - lr: 0.0010\n",
            "Epoch 74/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8440\n",
            "Epoch 00074: loss improved from 1.84893 to 1.84399, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8440 - lr: 0.0010\n",
            "Epoch 75/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.8382\n",
            "Epoch 00075: loss improved from 1.84399 to 1.83818, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8382 - lr: 0.0010\n",
            "Epoch 76/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8316\n",
            "Epoch 00076: loss improved from 1.83818 to 1.83155, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8316 - lr: 0.0010\n",
            "Epoch 77/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8288\n",
            "Epoch 00077: loss improved from 1.83155 to 1.82884, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8288 - lr: 0.0010\n",
            "Epoch 78/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8277\n",
            "Epoch 00078: loss improved from 1.82884 to 1.82766, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8277 - lr: 0.0010\n",
            "Epoch 79/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8200\n",
            "Epoch 00079: loss improved from 1.82766 to 1.82005, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8200 - lr: 0.0010\n",
            "Epoch 80/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.8139\n",
            "Epoch 00080: loss improved from 1.82005 to 1.81387, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8139 - lr: 0.0010\n",
            "Epoch 81/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8096\n",
            "Epoch 00081: loss improved from 1.81387 to 1.80958, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8096 - lr: 0.0010\n",
            "Epoch 82/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8074\n",
            "Epoch 00082: loss improved from 1.80958 to 1.80737, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.8074 - lr: 0.0010\n",
            "Epoch 83/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.8012\n",
            "Epoch 00083: loss improved from 1.80737 to 1.80124, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.8012 - lr: 0.0010\n",
            "Epoch 84/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7980\n",
            "Epoch 00084: loss improved from 1.80124 to 1.79829, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.7983 - lr: 0.0010\n",
            "Epoch 85/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7932\n",
            "Epoch 00085: loss improved from 1.79829 to 1.79318, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7932 - lr: 0.0010\n",
            "Epoch 86/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7904\n",
            "Epoch 00086: loss improved from 1.79318 to 1.79039, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7904 - lr: 0.0010\n",
            "Epoch 87/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7870\n",
            "Epoch 00087: loss improved from 1.79039 to 1.78702, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7870 - lr: 0.0010\n",
            "Epoch 88/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7837\n",
            "Epoch 00088: loss improved from 1.78702 to 1.78370, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7837 - lr: 0.0010\n",
            "Epoch 89/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7815\n",
            "Epoch 00089: loss improved from 1.78370 to 1.78167, saving model to predictingword.h5\n",
            "703/703 [==============================] - 26s 38ms/step - loss: 1.7817 - lr: 0.0010\n",
            "Epoch 90/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7735\n",
            "Epoch 00090: loss improved from 1.78167 to 1.77347, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7735 - lr: 0.0010\n",
            "Epoch 91/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7721\n",
            "Epoch 00091: loss improved from 1.77347 to 1.77211, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7721 - lr: 0.0010\n",
            "Epoch 92/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7724\n",
            "Epoch 00092: loss did not improve from 1.77211\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 1.7724 - lr: 0.0010\n",
            "Epoch 93/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7640\n",
            "Epoch 00093: loss improved from 1.77211 to 1.76436, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7644 - lr: 0.0010\n",
            "Epoch 94/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7602\n",
            "Epoch 00094: loss improved from 1.76436 to 1.76065, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7607 - lr: 0.0010\n",
            "Epoch 95/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7599\n",
            "Epoch 00095: loss improved from 1.76065 to 1.75988, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7599 - lr: 0.0010\n",
            "Epoch 96/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7569\n",
            "Epoch 00096: loss improved from 1.75988 to 1.75688, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7569 - lr: 0.0010\n",
            "Epoch 97/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7550\n",
            "Epoch 00097: loss improved from 1.75688 to 1.75497, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7550 - lr: 0.0010\n",
            "Epoch 98/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7493\n",
            "Epoch 00098: loss improved from 1.75497 to 1.74916, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7492 - lr: 0.0010\n",
            "Epoch 99/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7464\n",
            "Epoch 00099: loss improved from 1.74916 to 1.74642, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7464 - lr: 0.0010\n",
            "Epoch 100/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7450\n",
            "Epoch 00100: loss improved from 1.74642 to 1.74504, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7450 - lr: 0.0010\n",
            "Epoch 101/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7422\n",
            "Epoch 00101: loss improved from 1.74504 to 1.74198, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7420 - lr: 0.0010\n",
            "Epoch 102/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7378\n",
            "Epoch 00102: loss improved from 1.74198 to 1.73832, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7383 - lr: 0.0010\n",
            "Epoch 103/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7362\n",
            "Epoch 00103: loss improved from 1.73832 to 1.73621, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7362 - lr: 0.0010\n",
            "Epoch 104/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7321\n",
            "Epoch 00104: loss improved from 1.73621 to 1.73210, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.7321 - lr: 0.0010\n",
            "Epoch 105/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7280\n",
            "Epoch 00105: loss improved from 1.73210 to 1.72802, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7280 - lr: 0.0010\n",
            "Epoch 106/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7290\n",
            "Epoch 00106: loss did not improve from 1.72802\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.7290 - lr: 0.0010\n",
            "Epoch 107/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7282\n",
            "Epoch 00107: loss did not improve from 1.72802\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.7287 - lr: 0.0010\n",
            "Epoch 108/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7244\n",
            "Epoch 00108: loss improved from 1.72802 to 1.72448, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7245 - lr: 0.0010\n",
            "Epoch 109/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7222\n",
            "Epoch 00109: loss improved from 1.72448 to 1.72196, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7220 - lr: 0.0010\n",
            "Epoch 110/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7199\n",
            "Epoch 00110: loss improved from 1.72196 to 1.71991, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7199 - lr: 0.0010\n",
            "Epoch 111/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7174\n",
            "Epoch 00111: loss improved from 1.71991 to 1.71739, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7174 - lr: 0.0010\n",
            "Epoch 112/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7154\n",
            "Epoch 00112: loss improved from 1.71739 to 1.71543, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.7154 - lr: 0.0010\n",
            "Epoch 113/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7125\n",
            "Epoch 00113: loss improved from 1.71543 to 1.71247, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7125 - lr: 0.0010\n",
            "Epoch 114/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7106\n",
            "Epoch 00114: loss improved from 1.71247 to 1.71094, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.7109 - lr: 0.0010\n",
            "Epoch 115/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7071\n",
            "Epoch 00115: loss improved from 1.71094 to 1.70706, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7071 - lr: 0.0010\n",
            "Epoch 116/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7060\n",
            "Epoch 00116: loss improved from 1.70706 to 1.70585, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.7058 - lr: 0.0010\n",
            "Epoch 117/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.7043\n",
            "Epoch 00117: loss improved from 1.70585 to 1.70430, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7043 - lr: 0.0010\n",
            "Epoch 118/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7014\n",
            "Epoch 00118: loss improved from 1.70430 to 1.70145, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.7014 - lr: 0.0010\n",
            "Epoch 119/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7012\n",
            "Epoch 00119: loss improved from 1.70145 to 1.70144, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.7014 - lr: 0.0010\n",
            "Epoch 120/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.7000\n",
            "Epoch 00120: loss improved from 1.70144 to 1.69981, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6998 - lr: 0.0010\n",
            "Epoch 121/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6959\n",
            "Epoch 00121: loss improved from 1.69981 to 1.69591, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6959 - lr: 0.0010\n",
            "Epoch 122/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6956\n",
            "Epoch 00122: loss improved from 1.69591 to 1.69564, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6956 - lr: 0.0010\n",
            "Epoch 123/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6924\n",
            "Epoch 00123: loss improved from 1.69564 to 1.69241, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6924 - lr: 0.0010\n",
            "Epoch 124/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6907\n",
            "Epoch 00124: loss improved from 1.69241 to 1.69071, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6907 - lr: 0.0010\n",
            "Epoch 125/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6870\n",
            "Epoch 00125: loss improved from 1.69071 to 1.68699, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6870 - lr: 0.0010\n",
            "Epoch 126/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6845\n",
            "Epoch 00126: loss improved from 1.68699 to 1.68509, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6851 - lr: 0.0010\n",
            "Epoch 127/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6869\n",
            "Epoch 00127: loss did not improve from 1.68509\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.6869 - lr: 0.0010\n",
            "Epoch 128/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6840\n",
            "Epoch 00128: loss improved from 1.68509 to 1.68398, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6840 - lr: 0.0010\n",
            "Epoch 129/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6839\n",
            "Epoch 00129: loss improved from 1.68398 to 1.68386, saving model to predictingword.h5\n",
            "703/703 [==============================] - 28s 39ms/step - loss: 1.6839 - lr: 0.0010\n",
            "Epoch 130/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6807\n",
            "Epoch 00130: loss improved from 1.68386 to 1.68119, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6812 - lr: 0.0010\n",
            "Epoch 131/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6812\n",
            "Epoch 00131: loss did not improve from 1.68119\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.6812 - lr: 0.0010\n",
            "Epoch 132/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6756\n",
            "Epoch 00132: loss improved from 1.68119 to 1.67558, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6756 - lr: 0.0010\n",
            "Epoch 133/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6764\n",
            "Epoch 00133: loss did not improve from 1.67558\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.6764 - lr: 0.0010\n",
            "Epoch 134/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6740\n",
            "Epoch 00134: loss improved from 1.67558 to 1.67390, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6739 - lr: 0.0010\n",
            "Epoch 135/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6725\n",
            "Epoch 00135: loss improved from 1.67390 to 1.67236, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6724 - lr: 0.0010\n",
            "Epoch 136/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6717\n",
            "Epoch 00136: loss improved from 1.67236 to 1.67186, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6719 - lr: 0.0010\n",
            "Epoch 137/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6691\n",
            "Epoch 00137: loss improved from 1.67186 to 1.66952, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6695 - lr: 0.0010\n",
            "Epoch 138/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6687\n",
            "Epoch 00138: loss improved from 1.66952 to 1.66891, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6689 - lr: 0.0010\n",
            "Epoch 139/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6684\n",
            "Epoch 00139: loss improved from 1.66891 to 1.66828, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.6683 - lr: 0.0010\n",
            "Epoch 140/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6672\n",
            "Epoch 00140: loss improved from 1.66828 to 1.66681, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 39ms/step - loss: 1.6668 - lr: 0.0010\n",
            "Epoch 141/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6679\n",
            "Epoch 00141: loss did not improve from 1.66681\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.6678 - lr: 0.0010\n",
            "Epoch 142/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6625\n",
            "Epoch 00142: loss improved from 1.66681 to 1.66252, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.6625 - lr: 0.0010\n",
            "Epoch 143/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6627\n",
            "Epoch 00143: loss did not improve from 1.66252\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.6627 - lr: 0.0010\n",
            "Epoch 144/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6617\n",
            "Epoch 00144: loss improved from 1.66252 to 1.66132, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.6613 - lr: 0.0010\n",
            "Epoch 145/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6609\n",
            "Epoch 00145: loss improved from 1.66132 to 1.66088, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.6609 - lr: 0.0010\n",
            "Epoch 146/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6587\n",
            "Epoch 00146: loss improved from 1.66088 to 1.65877, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.6588 - lr: 0.0010\n",
            "Epoch 147/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6580\n",
            "Epoch 00147: loss improved from 1.65877 to 1.65811, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.6581 - lr: 0.0010\n",
            "Epoch 148/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6530\n",
            "Epoch 00148: loss improved from 1.65811 to 1.65297, saving model to predictingword.h5\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 1.6530 - lr: 0.0010\n",
            "Epoch 149/150\n",
            "702/703 [============================>.] - ETA: 0s - loss: 1.6536\n",
            "Epoch 00149: loss did not improve from 1.65297\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.6536 - lr: 0.0010\n",
            "Epoch 150/150\n",
            "703/703 [==============================] - ETA: 0s - loss: 1.6546\n",
            "Epoch 00150: loss did not improve from 1.65297\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.6546 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f16fe414f90>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HveklZBrkhqu"
      },
      "source": [
        "model.save('predictingword.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l423lilk2Wo",
        "outputId": "b8141e8d-136b-4fde-dbda-6633f64c9a65"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTh7X8CxmQ_3"
      },
      "source": [
        "model.save('predictingword.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4zaKQjpm8rJ"
      },
      "source": [
        "model.save('predictingword.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdCjg7Yjnnkl"
      },
      "source": [
        "path = F\"/content/gdrive/My Drive/NN-Project\"\n",
        "model.save('predictingword.h5', path) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "hN4zGhgeoCc7",
        "outputId": "a1fc8f31-8338-4b16-9fcd-59d45aab90e4"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"predictingword.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_69c3f42f-3a0c-4996-b6e5-9c1a80db3b92\", \"predictingword.h5\", 431898160)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}